{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw8h5YSfiMZx",
        "outputId": "387a3e54-4f41-4262-d1d1-6dfd95113032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\r0% [Waiting for headers] [1 InRelease 11.3 kB/110 kB 10%] [Connected to cloud.r\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [1 InRelease 22.9 kB/110 kB 21%] [Connected to cloud.r-project.org (18.160.2\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [1 InRelease 22.9 kB/110 kB 21%] [Connected to ppa.lau\r                                                                               \rGet:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [47.6 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [632 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,265 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,294 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,494 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,520 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,535 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [32.6 kB]\n",
            "Fetched 8,161 kB in 2s (4,923 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "python3-dev is already the newest version (3.10.6-1~22.04).\n",
            "python3-dev set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  python3-setuptools python3-wheel swig4.0\n",
            "Suggested packages:\n",
            "  python-numpy-doc python3-pytest python-setuptools-doc swig-doc swig-examples\n",
            "  swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  python3-numpy python3-pip python3-setuptools python3-wheel swig swig4.0\n",
            "0 upgraded, 6 newly installed, 0 to remove and 17 not upgraded.\n",
            "Need to get 6,260 kB of archives.\n",
            "After this operation, 33.4 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-numpy amd64 1:1.21.5-1ubuntu22.04.1 [3,467 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-setuptools all 59.6.0-1.2ubuntu0.22.04.1 [339 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.4 [1,305 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 6,260 kB in 0s (15.7 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 6.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python3-numpy.\n",
            "(Reading database ... 120882 files and directories currently installed.)\n",
            "Preparing to unpack .../0-python3-numpy_1%3a1.21.5-1ubuntu22.04.1_amd64.deb ...\n",
            "Unpacking python3-numpy (1:1.21.5-1ubuntu22.04.1) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../1-python3-setuptools_59.6.0-1.2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-setuptools (59.6.0-1.2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../2-python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../3-python3-pip_22.0.2+dfsg-1ubuntu0.4_all.deb ...\n",
            "Unpacking python3-pip (22.0.2+dfsg-1ubuntu0.4) ...\n",
            "Selecting previously unselected package swig4.0.\n",
            "Preparing to unpack .../4-swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../5-swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up python3-setuptools (59.6.0-1.2ubuntu0.22.04.1) ...\n",
            "Setting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Setting up python3-pip (22.0.2+dfsg-1ubuntu0.4) ...\n",
            "Setting up python3-numpy (1:1.21.5-1ubuntu22.04.1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-dev python3-pip build-essential python3-numpy swig\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJH77jJIuWUm",
        "outputId": "88d19aee-f3fd-4b73-bebe-c946182cc1e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium[box2d]\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[box2d])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.1.1.post0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349110 sha256=73b70ab35254bcc542a2e297d91b352398e629cd40489cf4c9b7238eea35a4d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, farama-notifications, box2d-py, gymnasium\n",
            "Successfully installed box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-0.29.1 swig-4.1.1.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym stable-baselines3[extra]\n",
        "\n",
        "#importing necessary libraries\n",
        "\n",
        "import gym\n",
        "from stable_baselines3 import PPO, A2C\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "# Creating environments for LunarLander and Acrobot\n",
        "lunarlander_env = make_vec_env('LunarLander-v2', n_envs=1)\n",
        "acrobot_env = make_vec_env('Acrobot-v1', n_envs=1)\n",
        "\n",
        "# Instantiating and training the PPO model for LunarLander\n",
        "lunarlander_model = PPO(\"MlpPolicy\", lunarlander_env, verbose=1)\n",
        "lunarlander_model.learn(total_timesteps=10000)\n",
        "\n",
        "# Training the A2C model for Acrobot\n",
        "acrobot_model = A2C(\"MlpPolicy\", acrobot_env, verbose=1)\n",
        "acrobot_model.learn(total_timesteps=10000)\n",
        "\n",
        "# Evaluating the LunarLander agent\n",
        "lunarlander_mean_reward, _ = evaluate_policy(lunarlander_model, lunarlander_env, n_eval_episodes=10)\n",
        "\n",
        "# Evaluating the Acrobot agent\n",
        "acrobot_mean_reward, _ = evaluate_policy(acrobot_model, acrobot_env, n_eval_episodes=10)\n",
        "\n",
        "# Plotting the results of Environments, Rewards and Comparison of the environments\n",
        "environments = ['LunarLander', 'Acrobot']\n",
        "rewards = [lunarlander_mean_reward, acrobot_mean_reward]\n",
        "\n",
        "plt.bar(environments, rewards)\n",
        "plt.xlabel('Environment')\n",
        "plt.ylabel('Mean Reward')\n",
        "plt.title('Performance of Trained Agents')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-jKtpVwVkoIP",
        "outputId": "f8158317-8511-4bce-fb7c-c94eb1b79720"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.2.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.7/181.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (0.29.1)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.1.0+cu118)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.14.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.7.0)\n",
            "Collecting shimmy[atari]~=1.3.0 (from stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Collecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (0.0.4)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.59.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.5.1)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2023.3.post1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]) (6.1.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=5445d1be336cdff5be021a540ae1236f8fa174517a4d4706a41f508bc6185708\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: ale-py, shimmy, AutoROM.accept-rom-license, autorom, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 shimmy-1.3.0 stable-baselines3-2.2.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/dtypes.py:35: DeprecationWarning: ml_dtypes.float8_e4m3b11 is deprecated. Use ml_dtypes.float8_e4m3b11fnuz\n",
            "  from tensorflow.tsl.python.lib.core import pywrap_ml_dtypes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 97.2     |\n",
            "|    ep_rew_mean     | -167     |\n",
            "| time/              |          |\n",
            "|    fps             | 1002     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 93.2        |\n",
            "|    ep_rew_mean          | -164        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 769         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 5           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009242356 |\n",
            "|    clip_fraction        | 0.033       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.38       |\n",
            "|    explained_variance   | -0.000403   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 356         |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0116     |\n",
            "|    value_loss           | 1.15e+03    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 95.8         |\n",
            "|    ep_rew_mean          | -150         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 653          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 9            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042753015 |\n",
            "|    clip_fraction        | 0.0227       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.37        |\n",
            "|    explained_variance   | -0.0115      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 466          |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00654     |\n",
            "|    value_loss           | 994          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 97.5        |\n",
            "|    ep_rew_mean          | -141        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 659         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 12          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007536364 |\n",
            "|    clip_fraction        | 0.0583      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.36       |\n",
            "|    explained_variance   | -0.0127     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 308         |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00633    |\n",
            "|    value_loss           | 650         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 99.8        |\n",
            "|    ep_rew_mean          | -136        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 659         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 15          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014563147 |\n",
            "|    clip_fraction        | 0.0773      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.34       |\n",
            "|    explained_variance   | -0.00452    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 203         |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00897    |\n",
            "|    value_loss           | 446         |\n",
            "-----------------------------------------\n",
            "Using cpu device\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 500      |\n",
            "|    ep_rew_mean        | -500     |\n",
            "| time/                 |          |\n",
            "|    fps                | 632      |\n",
            "|    iterations         | 100      |\n",
            "|    time_elapsed       | 0        |\n",
            "|    total_timesteps    | 500      |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.01    |\n",
            "|    explained_variance | -0.0244  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 99       |\n",
            "|    policy_loss        | -1.88    |\n",
            "|    value_loss         | 7.69     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 500      |\n",
            "|    ep_rew_mean        | -500     |\n",
            "| time/                 |          |\n",
            "|    fps                | 629      |\n",
            "|    iterations         | 200      |\n",
            "|    time_elapsed       | 1        |\n",
            "|    total_timesteps    | 1000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.06    |\n",
            "|    explained_variance | -0.357   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 199      |\n",
            "|    policy_loss        | -0.215   |\n",
            "|    value_loss         | 2.03     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 500      |\n",
            "|    ep_rew_mean        | -500     |\n",
            "| time/                 |          |\n",
            "|    fps                | 579      |\n",
            "|    iterations         | 300      |\n",
            "|    time_elapsed       | 2        |\n",
            "|    total_timesteps    | 1500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.975   |\n",
            "|    explained_variance | 0.0147   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 299      |\n",
            "|    policy_loss        | -2.49    |\n",
            "|    value_loss         | 4.56     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 454      |\n",
            "|    ep_rew_mean        | -454     |\n",
            "| time/                 |          |\n",
            "|    fps                | 540      |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 3        |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.08    |\n",
            "|    explained_variance | 0.026    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | -2.46    |\n",
            "|    value_loss         | 5.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 413      |\n",
            "|    ep_rew_mean        | -412     |\n",
            "| time/                 |          |\n",
            "|    fps                | 518      |\n",
            "|    iterations         | 500      |\n",
            "|    time_elapsed       | 4        |\n",
            "|    total_timesteps    | 2500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.06    |\n",
            "|    explained_variance | 0.000371 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 499      |\n",
            "|    policy_loss        | -2.32    |\n",
            "|    value_loss         | 5.05     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 385      |\n",
            "|    ep_rew_mean        | -384     |\n",
            "| time/                 |          |\n",
            "|    fps                | 509      |\n",
            "|    iterations         | 600      |\n",
            "|    time_elapsed       | 5        |\n",
            "|    total_timesteps    | 3000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.967   |\n",
            "|    explained_variance | -0.128   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 599      |\n",
            "|    policy_loss        | -2.16    |\n",
            "|    value_loss         | 4.7      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 334      |\n",
            "|    ep_rew_mean        | -334     |\n",
            "| time/                 |          |\n",
            "|    fps                | 522      |\n",
            "|    iterations         | 700      |\n",
            "|    time_elapsed       | 6        |\n",
            "|    total_timesteps    | 3500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.959   |\n",
            "|    explained_variance | 0.685    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 699      |\n",
            "|    policy_loss        | 0.26     |\n",
            "|    value_loss         | 0.345    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 306      |\n",
            "|    ep_rew_mean        | -305     |\n",
            "| time/                 |          |\n",
            "|    fps                | 535      |\n",
            "|    iterations         | 800      |\n",
            "|    time_elapsed       | 7        |\n",
            "|    total_timesteps    | 4000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.04    |\n",
            "|    explained_variance | 0.000723 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 799      |\n",
            "|    policy_loss        | -1.91    |\n",
            "|    value_loss         | 3.54     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 296      |\n",
            "|    ep_rew_mean        | -296     |\n",
            "| time/                 |          |\n",
            "|    fps                | 544      |\n",
            "|    iterations         | 900      |\n",
            "|    time_elapsed       | 8        |\n",
            "|    total_timesteps    | 4500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1       |\n",
            "|    explained_variance | -0.00257 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 899      |\n",
            "|    policy_loss        | -2.41    |\n",
            "|    value_loss         | 3        |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 284      |\n",
            "|    ep_rew_mean        | -283     |\n",
            "| time/                 |          |\n",
            "|    fps                | 552      |\n",
            "|    iterations         | 1000     |\n",
            "|    time_elapsed       | 9        |\n",
            "|    total_timesteps    | 5000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.01    |\n",
            "|    explained_variance | 0.0104   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 999      |\n",
            "|    policy_loss        | -1.32    |\n",
            "|    value_loss         | 2.6      |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 271       |\n",
            "|    ep_rew_mean        | -270      |\n",
            "| time/                 |           |\n",
            "|    fps                | 558       |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 9         |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.983    |\n",
            "|    explained_variance | -0.000941 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -1.41     |\n",
            "|    value_loss         | 2.1       |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 258      |\n",
            "|    ep_rew_mean        | -257     |\n",
            "| time/                 |          |\n",
            "|    fps                | 565      |\n",
            "|    iterations         | 1200     |\n",
            "|    time_elapsed       | 10       |\n",
            "|    total_timesteps    | 6000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.01    |\n",
            "|    explained_variance | 0.00056  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1199     |\n",
            "|    policy_loss        | -1.57    |\n",
            "|    value_loss         | 1.7      |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 257      |\n",
            "|    ep_rew_mean        | -256     |\n",
            "| time/                 |          |\n",
            "|    fps                | 571      |\n",
            "|    iterations         | 1300     |\n",
            "|    time_elapsed       | 11       |\n",
            "|    total_timesteps    | 6500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.08    |\n",
            "|    explained_variance | 0.000651 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1299     |\n",
            "|    policy_loss        | -1.11    |\n",
            "|    value_loss         | 1.35     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 261      |\n",
            "|    ep_rew_mean        | -260     |\n",
            "| time/                 |          |\n",
            "|    fps                | 573      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 12       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.02    |\n",
            "|    explained_variance | 0.00707  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | -0.813   |\n",
            "|    value_loss         | 1.04     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 265      |\n",
            "|    ep_rew_mean        | -264     |\n",
            "| time/                 |          |\n",
            "|    fps                | 572      |\n",
            "|    iterations         | 1500     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 7500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.03    |\n",
            "|    explained_variance | -0.00192 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1499     |\n",
            "|    policy_loss        | -0.864   |\n",
            "|    value_loss         | 0.756    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 273      |\n",
            "|    ep_rew_mean        | -272     |\n",
            "| time/                 |          |\n",
            "|    fps                | 575      |\n",
            "|    iterations         | 1600     |\n",
            "|    time_elapsed       | 13       |\n",
            "|    total_timesteps    | 8000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.08    |\n",
            "|    explained_variance | 0.204    |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1599     |\n",
            "|    policy_loss        | -0.578   |\n",
            "|    value_loss         | 0.398    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 278      |\n",
            "|    ep_rew_mean        | -277     |\n",
            "| time/                 |          |\n",
            "|    fps                | 579      |\n",
            "|    iterations         | 1700     |\n",
            "|    time_elapsed       | 14       |\n",
            "|    total_timesteps    | 8500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -0.908   |\n",
            "|    explained_variance | -0.00431 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1699     |\n",
            "|    policy_loss        | -0.522   |\n",
            "|    value_loss         | 0.335    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 272      |\n",
            "|    ep_rew_mean        | -271     |\n",
            "| time/                 |          |\n",
            "|    fps                | 582      |\n",
            "|    iterations         | 1800     |\n",
            "|    time_elapsed       | 15       |\n",
            "|    total_timesteps    | 9000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.03    |\n",
            "|    explained_variance | 0.000233 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1799     |\n",
            "|    policy_loss        | -0.411   |\n",
            "|    value_loss         | 0.193    |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| rollout/              |          |\n",
            "|    ep_len_mean        | 264      |\n",
            "|    ep_rew_mean        | -263     |\n",
            "| time/                 |          |\n",
            "|    fps                | 577      |\n",
            "|    iterations         | 1900     |\n",
            "|    time_elapsed       | 16       |\n",
            "|    total_timesteps    | 9500     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -1.09    |\n",
            "|    explained_variance | 0.000103 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1899     |\n",
            "|    policy_loss        | -0.276   |\n",
            "|    value_loss         | 0.0895   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| rollout/              |           |\n",
            "|    ep_len_mean        | 255       |\n",
            "|    ep_rew_mean        | -254      |\n",
            "| time/                 |           |\n",
            "|    fps                | 570       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 17        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -0.955    |\n",
            "|    explained_variance | -0.000159 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | -0.115    |\n",
            "|    value_loss         | 0.0253    |\n",
            "-------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stable-baselines3 gym[atari]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "id": "pj9n7LEfHuiS",
        "outputId": "77eaead5-2641-4ca1-a110-ece561b2528c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.1.0+cu118)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
            "Collecting ale-py~=0.7.5 (from gym[atari])\n",
            "  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[atari]) (6.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
            "Installing collected packages: ale-py\n",
            "  Attempting uninstall: ale-py\n",
            "    Found existing installation: ale-py 0.8.1\n",
            "    Uninstalling ale-py-0.8.1:\n",
            "      Successfully uninstalled ale-py-0.8.1\n",
            "Successfully installed ale-py-0.7.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ale_py",
                  "gym"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary libraries\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "# Defining the Atari environment name for Space Invaders\n",
        "env_id = 'SpaceInvadersNoFrameskip-v4'\n",
        "\n",
        "# Creating the Atari environment\n",
        "env = gym.make(env_id)\n",
        "\n",
        "# Defining the PPO model\n",
        "model = PPO(\"CnnPolicy\", env, verbose=1)\n",
        "\n",
        "# Training the model\n",
        "total_timesteps = 10000\n",
        "model.learn(total_timesteps=total_timesteps)\n",
        "\n",
        "# Evaluating the trained model\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR_ipF_cIsws",
        "outputId": "e31a62d8-91f1-424b-9df7-1cd364b2b817"
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.99e+03 |\n",
            "|    ep_rew_mean     | 70       |\n",
            "| time/              |          |\n",
            "|    fps             | 116      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 17       |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.99e+03   |\n",
            "|    ep_rew_mean          | 70         |\n",
            "| time/                   |            |\n",
            "|    fps                  | 19         |\n",
            "|    iterations           | 2          |\n",
            "|    time_elapsed         | 208        |\n",
            "|    total_timesteps      | 4096       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00843427 |\n",
            "|    clip_fraction        | 0.0469     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.78      |\n",
            "|    explained_variance   | 0.00449    |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.158      |\n",
            "|    n_updates            | 10         |\n",
            "|    policy_gradient_loss | -0.00954   |\n",
            "|    value_loss           | 1.67       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 2.2e+03     |\n",
            "|    ep_rew_mean          | 140         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 15          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 400         |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013069006 |\n",
            "|    clip_fraction        | 0.179       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.76       |\n",
            "|    explained_variance   | 0.126       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.734       |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00153    |\n",
            "|    value_loss           | 4.66        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 2.42e+03    |\n",
            "|    ep_rew_mean          | 180         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 13          |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 628         |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008162195 |\n",
            "|    clip_fraction        | 0.0854      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.76       |\n",
            "|    explained_variance   | 0.253       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.135       |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.00429    |\n",
            "|    value_loss           | 4.53        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 2.27e+03   |\n",
            "|    ep_rew_mean          | 174        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 12         |\n",
            "|    iterations           | 5          |\n",
            "|    time_elapsed         | 853        |\n",
            "|    total_timesteps      | 10240      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02008691 |\n",
            "|    clip_fraction        | 0.155      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.76      |\n",
            "|    explained_variance   | 0.554      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 2.44       |\n",
            "|    n_updates            | 40         |\n",
            "|    policy_gradient_loss | -0.0093    |\n",
            "|    value_loss           | 5.57       |\n",
            "----------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward: 730.00 +/- 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Defining the Atari environment name for Space Invaders\n",
        "env_id = 'SpaceInvadersNoFrameskip-v4'\n",
        "# Creating the Atari environment and wrap it with a Monitor\n",
        "env = gym.make(env_id)\n",
        "env = Monitor(env)\n",
        "\n",
        "# Defining the PPO model\n",
        "model = PPO(\"CnnPolicy\", env, verbose=1)\n",
        "\n",
        "# Training the model with multiple sets of total_timesteps\n",
        "total_timesteps_list = [5000, 10000, 20000]\n",
        "for idx, total_timesteps in enumerate(total_timesteps_list):\n",
        "    print(f\"Training {idx + 1}/{len(total_timesteps_list)}\")\n",
        "\n",
        "    # Setting up evaluation callback for each training run\n",
        "    eval_env = gym.make(env_id)\n",
        "    eval_callback = EvalCallback(eval_env, best_model_save_path=f'./logs/{idx}/',\n",
        "                                 log_path=f'./logs/{idx}/', eval_freq=500,\n",
        "                                 deterministic=True, render=False)\n",
        "\n",
        "    # Training the model\n",
        "    model.learn(total_timesteps=total_timesteps, callback=eval_callback)\n",
        "\n",
        "# Visualizing training progress using Tensorboard logs for each run\n",
        "for idx, _ in enumerate(total_timesteps_list):\n",
        "    !tensorboard --logdir ./logs/{idx}/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRY_JDctcBqM",
        "outputId": "c4cee11e-7d6b-4432-fdc8-8e24f01f6fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n",
            "Training 1/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x79336c86bc70> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7934a1273d00>\n",
            "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval num_timesteps=500, episode_reward=55.00 +/- 0.00\n",
            "Episode length: 2723.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.72e+03 |\n",
            "|    mean_reward     | 55       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 500      |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1000, episode_reward=55.00 +/- 0.00\n",
            "Episode length: 2723.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.72e+03 |\n",
            "|    mean_reward     | 55       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1500, episode_reward=55.00 +/- 0.00\n",
            "Episode length: 2723.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.72e+03 |\n",
            "|    mean_reward     | 55       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1500     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2000, episode_reward=55.00 +/- 0.00\n",
            "Episode length: 2723.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.72e+03 |\n",
            "|    mean_reward     | 55       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2000     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 4    |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 463  |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=2500, episode_reward=15.00 +/- 0.00\n",
            "Episode length: 2031.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 2.03e+03    |\n",
            "|    mean_reward          | 15          |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 2500        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011242735 |\n",
            "|    clip_fraction        | 0.0745      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.78       |\n",
            "|    explained_variance   | 0.00485     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.34        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00431    |\n",
            "|    value_loss           | 3.61        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3000, episode_reward=15.00 +/- 0.00\n",
            "Episode length: 2031.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.03e+03 |\n",
            "|    mean_reward     | 15       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3500, episode_reward=15.00 +/- 0.00\n",
            "Episode length: 2031.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.03e+03 |\n",
            "|    mean_reward     | 15       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3500     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4000, episode_reward=15.00 +/- 0.00\n",
            "Episode length: 2031.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.03e+03 |\n",
            "|    mean_reward     | 15       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2.22e+03 |\n",
            "|    ep_rew_mean     | 105      |\n",
            "| time/              |          |\n",
            "|    fps             | 4        |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 973      |\n",
            "|    total_timesteps | 4096     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4500, episode_reward=125.00 +/- 0.00\n",
            "Episode length: 2747.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 2.75e+03    |\n",
            "|    mean_reward          | 125         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4500        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012299169 |\n",
            "|    clip_fraction        | 0.1         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.78       |\n",
            "|    explained_variance   | 0.152       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.615       |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.00392    |\n",
            "|    value_loss           | 2.5         |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=5000, episode_reward=125.00 +/- 0.00\n",
            "Episode length: 2747.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.75e+03 |\n",
            "|    mean_reward     | 125      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5500, episode_reward=125.00 +/- 0.00\n",
            "Episode length: 2747.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.75e+03 |\n",
            "|    mean_reward     | 125      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5500     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=6000, episode_reward=125.00 +/- 0.00\n",
            "Episode length: 2747.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.75e+03 |\n",
            "|    mean_reward     | 125      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 6000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.96e+03 |\n",
            "|    ep_rew_mean     | 100      |\n",
            "| time/              |          |\n",
            "|    fps             | 3        |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 1598     |\n",
            "|    total_timesteps | 6144     |\n",
            "---------------------------------\n",
            "Training 2/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x79336c86bc70> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x79336beeccd0>\n",
            "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval num_timesteps=500, episode_reward=50.00 +/- 0.00\n",
            "Episode length: 1015.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.02e+03 |\n",
            "|    mean_reward     | 50       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 500      |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1000, episode_reward=50.00 +/- 0.00\n",
            "Episode length: 1015.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.02e+03 |\n",
            "|    mean_reward     | 50       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1500, episode_reward=50.00 +/- 0.00\n",
            "Episode length: 1015.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.02e+03 |\n",
            "|    mean_reward     | 50       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1500     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2000, episode_reward=50.00 +/- 0.00\n",
            "Episode length: 1015.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.02e+03 |\n",
            "|    mean_reward     | 50       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2000     |\n",
            "---------------------------------\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 11   |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 175  |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "Eval num_timesteps=2500, episode_reward=235.00 +/- 0.00\n",
            "Episode length: 2525.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 2.52e+03    |\n",
            "|    mean_reward          | 235         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 2500        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010737563 |\n",
            "|    clip_fraction        | 0.174       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.75       |\n",
            "|    explained_variance   | 0.137       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.26        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.00897    |\n",
            "|    value_loss           | 4.93        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=3000, episode_reward=235.00 +/- 0.00\n",
            "Episode length: 2525.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.52e+03 |\n",
            "|    mean_reward     | 235      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3500, episode_reward=235.00 +/- 0.00\n",
            "Episode length: 2525.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.52e+03 |\n",
            "|    mean_reward     | 235      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3500     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4000, episode_reward=235.00 +/- 0.00\n",
            "Episode length: 2525.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.52e+03 |\n",
            "|    mean_reward     | 235      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.78e+03 |\n",
            "|    ep_rew_mean     | 168      |\n",
            "| time/              |          |\n",
            "|    fps             | 5        |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 791      |\n",
            "|    total_timesteps | 4096     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4500, episode_reward=210.00 +/- 0.00\n",
            "Episode length: 2327.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 2.33e+03    |\n",
            "|    mean_reward          | 210         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4500        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026609188 |\n",
            "|    clip_fraction        | 0.266       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.73       |\n",
            "|    explained_variance   | 0.452       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.99        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0109     |\n",
            "|    value_loss           | 5.05        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=5000, episode_reward=210.00 +/- 0.00\n",
            "Episode length: 2327.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.33e+03 |\n",
            "|    mean_reward     | 210      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5500, episode_reward=210.00 +/- 0.00\n",
            "Episode length: 2327.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.33e+03 |\n",
            "|    mean_reward     | 210      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5500     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=6000, episode_reward=210.00 +/- 0.00\n",
            "Episode length: 2327.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.33e+03 |\n",
            "|    mean_reward     | 210      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 6000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.81e+03 |\n",
            "|    ep_rew_mean     | 153      |\n",
            "| time/              |          |\n",
            "|    fps             | 4        |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 1389     |\n",
            "|    total_timesteps | 6144     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=6500, episode_reward=180.00 +/- 0.00\n",
            "Episode length: 1969.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1.97e+03    |\n",
            "|    mean_reward          | 180         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 6500        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013905337 |\n",
            "|    clip_fraction        | 0.19        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.72       |\n",
            "|    explained_variance   | 0.653       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.404       |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0156     |\n",
            "|    value_loss           | 1.4         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=7000, episode_reward=180.00 +/- 0.00\n",
            "Episode length: 1969.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.97e+03 |\n",
            "|    mean_reward     | 180      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 7000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=7500, episode_reward=180.00 +/- 0.00\n",
            "Episode length: 1969.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.97e+03 |\n",
            "|    mean_reward     | 180      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 7500     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=8000, episode_reward=180.00 +/- 0.00\n",
            "Episode length: 1969.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.97e+03 |\n",
            "|    mean_reward     | 180      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.84e+03 |\n",
            "|    ep_rew_mean     | 145      |\n",
            "| time/              |          |\n",
            "|    fps             | 4        |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 1936     |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=8500, episode_reward=160.00 +/- 0.00\n",
            "Episode length: 1655.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1.66e+03    |\n",
            "|    mean_reward          | 160         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 8500        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015629463 |\n",
            "|    clip_fraction        | 0.202       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.71       |\n",
            "|    explained_variance   | 0.684       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.704       |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.016      |\n",
            "|    value_loss           | 2.47        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=9000, episode_reward=160.00 +/- 0.00\n",
            "Episode length: 1655.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.66e+03 |\n",
            "|    mean_reward     | 160      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 9000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=9500, episode_reward=160.00 +/- 0.00\n",
            "Episode length: 1655.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.66e+03 |\n",
            "|    mean_reward     | 160      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 9500     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=160.00 +/- 0.00\n",
            "Episode length: 1655.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.66e+03 |\n",
            "|    mean_reward     | 160      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 10000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.68e+03 |\n",
            "|    ep_rew_mean     | 121      |\n",
            "| time/              |          |\n",
            "|    fps             | 4        |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 2432     |\n",
            "|    total_timesteps | 10240    |\n",
            "---------------------------------\n",
            "Training 3/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py:414: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x79336c86bc70> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x79336beec280>\n",
            "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval num_timesteps=500, episode_reward=220.00 +/- 0.00\n",
            "Episode length: 3715.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 3.72e+03 |\n",
            "|    mean_reward     | 220      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 500      |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1000, episode_reward=220.00 +/- 0.00\n",
            "Episode length: 3715.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 3.72e+03 |\n",
            "|    mean_reward     | 220      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1500, episode_reward=220.00 +/- 0.00\n",
            "Episode length: 3715.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 3.72e+03 |\n",
            "|    mean_reward     | 220      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1500     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2000, episode_reward=220.00 +/- 0.00\n",
            "Episode length: 3715.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 3.72e+03 |\n",
            "|    mean_reward     | 220      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.5e+03  |\n",
            "|    ep_rew_mean     | 85       |\n",
            "| time/              |          |\n",
            "|    fps             | 3        |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 615      |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2500, episode_reward=210.00 +/- 0.00\n",
            "Episode length: 2483.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 2.48e+03    |\n",
            "|    mean_reward          | 210         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 2500        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019541696 |\n",
            "|    clip_fraction        | 0.176       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.71       |\n",
            "|    explained_variance   | 0.683       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.658       |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0132     |\n",
            "|    value_loss           | 2.61        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=3000, episode_reward=210.00 +/- 0.00\n",
            "Episode length: 2483.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.48e+03 |\n",
            "|    mean_reward     | 210      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3500, episode_reward=210.00 +/- 0.00\n",
            "Episode length: 2483.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.48e+03 |\n",
            "|    mean_reward     | 210      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3500     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4000, episode_reward=210.00 +/- 0.00\n",
            "Episode length: 2483.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.48e+03 |\n",
            "|    mean_reward     | 210      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.5e+03  |\n",
            "|    ep_rew_mean     | 85       |\n",
            "| time/              |          |\n",
            "|    fps             | 3        |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 1247     |\n",
            "|    total_timesteps | 4096     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4500, episode_reward=190.00 +/- 0.00\n",
            "Episode length: 2829.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 2.83e+03    |\n",
            "|    mean_reward          | 190         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 4500        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016615132 |\n",
            "|    clip_fraction        | 0.208       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.7        |\n",
            "|    explained_variance   | 0.523       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.239       |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0133     |\n",
            "|    value_loss           | 2.6         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=5000, episode_reward=190.00 +/- 0.00\n",
            "Episode length: 2829.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.83e+03 |\n",
            "|    mean_reward     | 190      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5500, episode_reward=190.00 +/- 0.00\n",
            "Episode length: 2829.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.83e+03 |\n",
            "|    mean_reward     | 190      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5500     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=6000, episode_reward=190.00 +/- 0.00\n",
            "Episode length: 2829.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.83e+03 |\n",
            "|    mean_reward     | 190      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 6000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2.68e+03 |\n",
            "|    ep_rew_mean     | 188      |\n",
            "| time/              |          |\n",
            "|    fps             | 3        |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 1919     |\n",
            "|    total_timesteps | 6144     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=6500, episode_reward=175.00 +/- 0.00\n",
            "Episode length: 1987.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 1.99e+03    |\n",
            "|    mean_reward          | 175         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 6500        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016737618 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.72       |\n",
            "|    explained_variance   | 0.592       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.684       |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.0182     |\n",
            "|    value_loss           | 2.58        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=7000, episode_reward=175.00 +/- 0.00\n",
            "Episode length: 1987.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.99e+03 |\n",
            "|    mean_reward     | 175      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 7000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=7500, episode_reward=175.00 +/- 0.00\n",
            "Episode length: 1987.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.99e+03 |\n",
            "|    mean_reward     | 175      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 7500     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=8000, episode_reward=175.00 +/- 0.00\n",
            "Episode length: 1987.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.99e+03 |\n",
            "|    mean_reward     | 175      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2.68e+03 |\n",
            "|    ep_rew_mean     | 215      |\n",
            "| time/              |          |\n",
            "|    fps             | 3        |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 2445     |\n",
            "|    total_timesteps | 8192     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=8500, episode_reward=175.00 +/- 0.00\n",
            "Episode length: 2819.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 2.82e+03    |\n",
            "|    mean_reward          | 175         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 8500        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026433796 |\n",
            "|    clip_fraction        | 0.272       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.68       |\n",
            "|    explained_variance   | 0.289       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.07        |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.00521    |\n",
            "|    value_loss           | 5.89        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=9000, episode_reward=175.00 +/- 0.00\n",
            "Episode length: 2819.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.82e+03 |\n",
            "|    mean_reward     | 175      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 9000     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=9500, episode_reward=175.00 +/- 0.00\n",
            "Episode length: 2819.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.82e+03 |\n",
            "|    mean_reward     | 175      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 9500     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=175.00 +/- 0.00\n",
            "Episode length: 2819.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.82e+03 |\n",
            "|    mean_reward     | 175      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 10000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2.33e+03 |\n",
            "|    ep_rew_mean     | 181      |\n",
            "| time/              |          |\n",
            "|    fps             | 3        |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 3100     |\n",
            "|    total_timesteps | 10240    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=10500, episode_reward=190.00 +/- 0.00\n",
            "Episode length: 2089.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 2.09e+03    |\n",
            "|    mean_reward          | 190         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 10500       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026284155 |\n",
            "|    clip_fraction        | 0.253       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.61       |\n",
            "|    explained_variance   | 0.776       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.435       |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.0117     |\n",
            "|    value_loss           | 1.95        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=11000, episode_reward=190.00 +/- 0.00\n",
            "Episode length: 2089.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.09e+03 |\n",
            "|    mean_reward     | 190      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 11000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=11500, episode_reward=190.00 +/- 0.00\n",
            "Episode length: 2089.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.09e+03 |\n",
            "|    mean_reward     | 190      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 11500    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=12000, episode_reward=190.00 +/- 0.00\n",
            "Episode length: 2089.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.09e+03 |\n",
            "|    mean_reward     | 190      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 12000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2.42e+03 |\n",
            "|    ep_rew_mean     | 182      |\n",
            "| time/              |          |\n",
            "|    fps             | 3        |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 3651     |\n",
            "|    total_timesteps | 12288    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=12500, episode_reward=155.00 +/- 0.00\n",
            "Episode length: 1729.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 1.73e+03   |\n",
            "|    mean_reward          | 155        |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 12500      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02094591 |\n",
            "|    clip_fraction        | 0.214      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.65      |\n",
            "|    explained_variance   | 0.435      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.622      |\n",
            "|    n_updates            | 140        |\n",
            "|    policy_gradient_loss | -0.0184    |\n",
            "|    value_loss           | 3.19       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=13000, episode_reward=155.00 +/- 0.00\n",
            "Episode length: 1729.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.73e+03 |\n",
            "|    mean_reward     | 155      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 13000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=13500, episode_reward=155.00 +/- 0.00\n",
            "Episode length: 1729.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.73e+03 |\n",
            "|    mean_reward     | 155      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 13500    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=14000, episode_reward=155.00 +/- 0.00\n",
            "Episode length: 1729.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1.73e+03 |\n",
            "|    mean_reward     | 155      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 14000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2.2e+03  |\n",
            "|    ep_rew_mean     | 158      |\n",
            "| time/              |          |\n",
            "|    fps             | 3        |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 4143     |\n",
            "|    total_timesteps | 14336    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=14500, episode_reward=220.00 +/- 0.00\n",
            "Episode length: 2503.00 +/- 0.00\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 2.5e+03    |\n",
            "|    mean_reward          | 220        |\n",
            "| time/                   |            |\n",
            "|    total_timesteps      | 14500      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02076032 |\n",
            "|    clip_fraction        | 0.24       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.62      |\n",
            "|    explained_variance   | 0.759      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.509      |\n",
            "|    n_updates            | 150        |\n",
            "|    policy_gradient_loss | -0.0136    |\n",
            "|    value_loss           | 2.26       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=15000, episode_reward=220.00 +/- 0.00\n",
            "Episode length: 2503.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.5e+03  |\n",
            "|    mean_reward     | 220      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 15000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=15500, episode_reward=220.00 +/- 0.00\n",
            "Episode length: 2503.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.5e+03  |\n",
            "|    mean_reward     | 220      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 15500    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16000, episode_reward=220.00 +/- 0.00\n",
            "Episode length: 2503.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.5e+03  |\n",
            "|    mean_reward     | 220      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 16000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2.12e+03 |\n",
            "|    ep_rew_mean     | 151      |\n",
            "| time/              |          |\n",
            "|    fps             | 3        |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 4754     |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=16500, episode_reward=220.00 +/- 0.00\n",
            "Episode length: 2501.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 2.5e+03     |\n",
            "|    mean_reward          | 220         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 16500       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024735458 |\n",
            "|    clip_fraction        | 0.229       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.62       |\n",
            "|    explained_variance   | 0.419       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.799       |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.009      |\n",
            "|    value_loss           | 2.91        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=17000, episode_reward=220.00 +/- 0.00\n",
            "Episode length: 2501.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.5e+03  |\n",
            "|    mean_reward     | 220      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 17000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=17500, episode_reward=220.00 +/- 0.00\n",
            "Episode length: 2501.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.5e+03  |\n",
            "|    mean_reward     | 220      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 17500    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=18000, episode_reward=220.00 +/- 0.00\n",
            "Episode length: 2501.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 2.5e+03  |\n",
            "|    mean_reward     | 220      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 18000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2.04e+03 |\n",
            "|    ep_rew_mean     | 147      |\n",
            "| time/              |          |\n",
            "|    fps             | 3        |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 5359     |\n",
            "|    total_timesteps | 18432    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=18500, episode_reward=550.00 +/- 0.00\n",
            "Episode length: 3621.00 +/- 0.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 3.62e+03    |\n",
            "|    mean_reward          | 550         |\n",
            "| time/                   |             |\n",
            "|    total_timesteps      | 18500       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023351986 |\n",
            "|    clip_fraction        | 0.255       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.63       |\n",
            "|    explained_variance   | 0.459       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.976       |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.0145     |\n",
            "|    value_loss           | 3.56        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=19000, episode_reward=550.00 +/- 0.00\n",
            "Episode length: 3621.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 3.62e+03 |\n",
            "|    mean_reward     | 550      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 19000    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=19500, episode_reward=550.00 +/- 0.00\n",
            "Episode length: 3621.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 3.62e+03 |\n",
            "|    mean_reward     | 550      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 19500    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=550.00 +/- 0.00\n",
            "Episode length: 3621.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 3.62e+03 |\n",
            "|    mean_reward     | 550      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 20000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 2.04e+03 |\n",
            "|    ep_rew_mean     | 147      |\n",
            "| time/              |          |\n",
            "|    fps             | 3        |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 6128     |\n",
            "|    total_timesteps | 20480    |\n",
            "---------------------------------\n",
            "2023-12-01 02:57:42.572539: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-01 02:57:42.572620: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-01 02:57:42.572685: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-01 02:57:44.433133: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.14.1 at http://localhost:6006/ (Press CTRL+C to quit)\n"
          ]
        }
      ]
    }
  ]
}